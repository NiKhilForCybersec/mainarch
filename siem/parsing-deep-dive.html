<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Parsing Deep Dive â€” Splunk vs Sentinel | Log Engineering</title>
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:wght@400;500;600;700&family=IBM+Plex+Mono:wght@400;500&display=swap" rel="stylesheet">
  <link rel="stylesheet" href="../css/styles.css">
  <style>
    .parse-stage { padding: 1rem; border-radius: 8px; margin: 0.5rem 0; }
    .parse-stage.input { background: rgba(59, 130, 246, 0.1); border-left: 4px solid #3b82f6; }
    .parse-stage.parse { background: rgba(249, 115, 22, 0.1); border-left: 4px solid #f97316; }
    .parse-stage.index { background: rgba(34, 197, 94, 0.1); border-left: 4px solid #22c55e; }
    .parse-stage.search { background: rgba(168, 85, 247, 0.1); border-left: 4px solid #a855f7; }
    .compare-grid { display: grid; grid-template-columns: 1fr 1fr; gap: 1.5rem; margin: 1.5rem 0; }
    .compare-card { background: var(--bg-secondary); border: 1px solid var(--border-primary); border-radius: 8px; padding: 1.25rem; }
    .compare-card h4 { margin: 0 0 1rem 0; padding-bottom: 0.5rem; border-bottom: 1px solid var(--border-primary); }
    .compare-card.splunk { border-top: 3px solid #65a637; }
    .compare-card.sentinel { border-top: 3px solid #0078d4; }
    @media (max-width: 900px) { .compare-grid { grid-template-columns: 1fr; } }
    .flow-diagram { background: var(--bg-tertiary); padding: 1.5rem; border-radius: 8px; margin: 1rem 0; overflow-x: auto; }
  </style>
</head>
<body>
  <div class="layout">
    <aside class="sidebar">
      <div class="sidebar-header">
        <div class="sidebar-logo">
          <div class="sidebar-logo-icon">RF</div>
          <div class="sidebar-logo-text">
            <span class="sidebar-logo-title">Security Transformation</span>
            <span class="sidebar-logo-subtitle">RevFlow Analytics</span>
          </div>
        </div>
      </div>
      <nav class="sidebar-nav">
        <div class="nav-section">
          <div class="nav-section-title">Overview</div>
          <a href="../index.html" class="nav-item">Executive Summary</a>
          <a href="../company-profile.html" class="nav-item">Company Profile</a>
          <a href="../security-framework.html" class="nav-item">Security Framework</a>
          <a href="../program-structure.html" class="nav-item">Program Structure</a>
          <a href="../controls-mapping.html" class="nav-item">Controls Mapping</a>
        </div>
        <div class="nav-divider"></div>
        <div class="nav-section">
          <div class="nav-section-title">SIEM Migration</div>
          <a href="../siem/index.html" class="nav-item">SIEM Overview</a>
          <a href="../siem/discovery.html" class="nav-item">Discovery & Assessment</a>
          <a href="../siem/log-sources.html" class="nav-item">Log Source Strategy</a>
          <a href="../siem/log-engineering.html" class="nav-item">Log Engineering</a>
          <a href="../siem/parsing-deep-dive.html" class="nav-item">Parsing Deep Dive</a>
          <a href="../siem/threat-intelligence.html" class="nav-item">Threat Intelligence</a>
          <a href="../siem/detection-engineering.html" class="nav-item">Detection Engineering</a>
          <a href="../siem/detection-anatomy.html" class="nav-item">Detection Anatomy</a>
          <a href="../siem/detection-catalog.html" class="nav-item">Detection Catalog</a>
          <a href="../siem/soar-automation.html" class="nav-item">SOAR &amp; Automation</a>
          <a href="../siem/workbooks.html" class="nav-item">Workbooks &amp; Dashboards</a>
          <a href="../siem/cost-optimization.html" class="nav-item">Cost Optimization</a>
          <a href="../siem/casb-saas-security.html" class="nav-item">CASB &amp; SaaS Security</a>
          <a href="../siem/soc-transition.html" class="nav-item">SOC Transition</a>
        </div>
        <div class="nav-divider"></div>
        <div class="nav-section">
          <div class="nav-section-title">EDR Migration</div>
          <a href="../edr/index.html" class="nav-item">EDR Overview</a>
          <a href="../edr/xdr-detection-model.html" class="nav-item">XDR Detection Model</a>
          <a href="../edr/advanced-hunting.html" class="nav-item">Advanced Hunting</a>
          <a href="../edr/live-response.html" class="nav-item">Live Response</a>
          <a href="../edr/tvm.html" class="nav-item">Vulnerability Management</a>
          <a href="../edr/deployment.html" class="nav-item">MDE Deployment</a>
          <a href="../edr/asr-rules.html" class="nav-item">ASR Rules</a>
          <a href="../edr/device-control.html" class="nav-item">Device Control</a>
          <a href="../edr/compensating-controls.html" class="nav-item">Compensating Controls</a>
          <a href="../edr/coexistence.html" class="nav-item">Coexistence Strategy</a>
          <a href="../edr/eset-removal.html" class="nav-item">ESET Removal</a>
        </div>
        <div class="nav-divider"></div>
        <div class="nav-section">
          <div class="nav-section-title">DLP Migration</div>
          <a href="../dlp/index.html" class="nav-item">DLP Overview</a>
          <a href="../dlp/dlp-catalog.html" class="nav-item">DLP Catalog</a>
          <a href="../dlp/policy-translation.html" class="nav-item">Policy Translation</a>
          <a href="../dlp/rollout-phases.html" class="nav-item">Rollout Phases</a>
          <a href="../dlp/user-communication.html" class="nav-item">User Communication</a>
        </div>
        <div class="nav-divider"></div>
        <div class="nav-section">
          <div class="nav-section-title">Identity & Access</div>
          <a href="../identity/index.html" class="nav-item">Identity Overview</a>
          <a href="../identity/hybrid-setup.html" class="nav-item">Hybrid Identity</a>
          <a href="../identity/conditional-access.html" class="nav-item">Conditional Access</a>
          <a href="../identity/pim.html" class="nav-item">PIM Configuration</a>
        </div>
        <div class="nav-divider"></div>
        <div class="nav-section">
          <div class="nav-section-title">Infrastructure</div>
          <a href="../infrastructure/index.html" class="nav-item">Infrastructure Overview</a>
          <a href="../infrastructure/landing-zone.html" class="nav-item">Landing Zone</a>
          <a href="../infrastructure/network-architecture.html" class="nav-item">Network Architecture</a>
          <a href="../infrastructure/workload-migration.html" class="nav-item">Workload Migration</a>
        </div>
        <div class="nav-divider"></div>
        <div class="nav-section">
          <div class="nav-section-title">Operations</div>
          <a href="../operations/parallel-ops.html" class="nav-item">Parallel Operations</a>
          <a href="../operations/soc-workflows.html" class="nav-item">SOC Workflows</a>
          <a href="../operations/integration.html" class="nav-item">Platform Integration</a>
          <a href="../operations/xdr-integration.html" class="nav-item">XDR Integration</a>
        </div>
        <div class="nav-divider"></div>
        <div class="nav-section">
          <div class="nav-section-title">Resources</div>
          <a href="../resources/timeline.html" class="nav-item">Project Timeline</a>
          <a href="../resources/raci.html" class="nav-item">RACI Matrix</a>
          <a href="../resources/infrastructure-reference.html" class="nav-item">Infrastructure Reference</a>
          <a href="../resources/scripts.html" class="nav-item">Scripts & Queries</a>
          <a href="../resources/data-dictionary.html" class="nav-item">Data Dictionary</a>
          <a href="../resources/templates.html" class="nav-item">Templates</a>
          <a href="../resources/splunk-reference.html" class="nav-item">Splunk Reference</a>
          <a href="../resources/alternative-architectures.html" class="nav-item">Alternative Architectures</a>
        </div>
        <div class="nav-divider"></div>
        <div class="nav-section">
          <div class="nav-section-title">Runbooks</div>
          <a href="../resources/mde-onboarding.html" class="nav-item">MDE Onboarding</a>
          <a href="../resources/mde-offboarding.html" class="nav-item">MDE Offboarding</a>
          <a href="../resources/incident-response.html" class="nav-item">Incident Response</a>
          <a href="../resources/operational-procedures.html" class="nav-item">Operational Procedures</a>
        </div>
        <div class="nav-divider"></div>
        <div class="nav-section">
          <div class="nav-section-title">Troubleshooting</div>
          <a href="../troubleshooting/index.html" class="nav-item">Common Issues</a>
          <a href="../troubleshooting/siem-issues.html" class="nav-item">SIEM Issues</a>
          <a href="../troubleshooting/mde-issues.html" class="nav-item">MDE Issues</a>
          <a href="../troubleshooting/dlp-issues.html" class="nav-item">DLP Issues</a>
          <a href="../troubleshooting/agent-conflicts.html" class="nav-item">Agent Conflicts</a>
        </div>
        <div class="nav-divider"></div>
        <div class="nav-section">
          <div class="nav-section-title">â˜… Interview Prep</div>
          <a href="../interview-behavioral.html" class="nav-item">Behavioral Questions</a>
          <a href="../interview-technical.html" class="nav-item">Technical Questions</a>
          <a href="../interview-team-fit.html" class="nav-item">Team Fit &amp; Director</a>
        </div>
        <div class="nav-divider"></div>
        <div class="nav-section">
          <div class="nav-section-title">Career &amp; Reference</div>
          <a href="../career-narrative.html" class="nav-item">Career Narrative</a>
          
          
          
          
          <a href="../director-interview-prep.html" class="nav-item">Director Prep (Full)</a>
          <a href="../soc-maturity.html" class="nav-item">SOC Maturity</a>
          <a href="../security-mental-model.html" class="nav-item">Security Mental Model</a>
          <a href="../ai-security-risk.html" class="nav-item">AI Security & Risk</a>
        </div>
      </nav>
    </aside>
    <button class="mobile-menu-toggle" aria-label="Toggle menu">
      <svg width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2"><path d="M3 12h18M3 6h18M3 18h18"></path></svg>
    </button>
    <div class="sidebar-overlay"></div>
    <main class="main-content">
      <div class="content-wrapper">
        <nav class="breadcrumb">
          <a href="../index.html">Home</a>
          <span class="breadcrumb-separator">/</span>
          <a href="index.html">SIEM Migration</a>
          <span class="breadcrumb-separator">/</span>
          <span class="breadcrumb-current">Parsing Deep Dive</span>
        </nav>

        <div class="page-header">
          <h1>Parsing Deep Dive â€” Splunk vs Sentinel</h1>
          <div class="page-header-meta">
            <span class="page-badge siem">SIEM Track</span>
            <span class="page-reading-time">Advanced Technical</span>
          </div>
          <p class="intro-text">This page provides step-by-step parsing implementation for both platforms. Understand exactly when, where, and how log parsing happens â€” from raw bytes to searchable fields.</p>
        </div>

        <!-- ==================== SECTION 1: PARSING OVERVIEW ==================== -->
        <h2>1. What Is Parsing and Why Does It Matter?</h2>

        <div class="callout info">
          <div class="callout-title"><svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm1 15h-2v-6h2v6zm0-8h-2V7h2v2z"/></svg>Definition</div>
          <div class="callout-content">
            <p><strong>Parsing</strong> = Converting raw log data into structured, searchable fields.</p>
            <p>Raw log: <code>Jan 15 10:30:45 web01 sshd[12345]: Failed password for invalid user admin from 192.168.1.100 port 54321</code></p>
            <p>Parsed fields: <code>timestamp=Jan 15 10:30:45, host=web01, process=sshd, pid=12345, action=Failed, user=admin, src_ip=192.168.1.100, src_port=54321</code></p>
          </div>
        </div>

        <p>Without parsing, you can only do full-text search. With parsing, you can do:</p>
        <ul>
          <li><strong>Field-based queries:</strong> <code>src_ip == "192.168.1.100"</code></li>
          <li><strong>Aggregations:</strong> <code>count() by user</code></li>
          <li><strong>Correlations:</strong> Join logs on common fields</li>
          <li><strong>Efficient storage:</strong> Typed fields compress better</li>
        </ul>

        <!-- ==================== SECTION 2: FUNDAMENTAL DIFFERENCE ==================== -->
        <h2>2. Fundamental Architectural Difference</h2>

        <div class="compare-grid">
          <div class="compare-card splunk">
            <h4>ğŸŸ¢ Splunk â€” Customer Controls Parsing</h4>
            <p><strong>You write the parsing logic.</strong></p>
            <ul>
              <li>Configure props.conf and transforms.conf</li>
              <li>Choose when parsing happens (index-time vs search-time)</li>
              <li>Choose where parsing happens (Heavy Forwarder vs Indexer)</li>
              <li>Full control = full responsibility</li>
            </ul>
            <p style="margin-top: 1rem;"><strong>Parsing locations:</strong></p>
            <ul>
              <li>Heavy Forwarder (before indexing)</li>
              <li>Indexer (during indexing)</li>
              <li>Search Head (during query)</li>
            </ul>
          </div>
          <div class="compare-card sentinel">
            <h4>ğŸ”µ Sentinel â€” Microsoft Controls Most Parsing</h4>
            <p><strong>Microsoft provides most parsing.</strong></p>
            <ul>
              <li>Native connectors parse automatically</li>
              <li>Standard tables have defined schemas</li>
              <li>Custom parsing via DCR, Functions, or KQL</li>
              <li>Less control, less work</li>
            </ul>
            <p style="margin-top: 1rem;"><strong>Parsing locations:</strong></p>
            <ul>
              <li>DCR transformation (ingestion-time)</li>
              <li>Azure Function (pre-ingestion)</li>
              <li>KQL query (search-time)</li>
            </ul>
          </div>
        </div>

        <!-- ==================== SECTION 3: SPLUNK PARSING DEEP DIVE ==================== -->
        <h2>3. Splunk Parsing â€” Step by Step</h2>

        <h3>3.1 The Four Parsing Stages</h3>

        <div class="flow-diagram">
          <pre><code>RAW LOG DATA
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: INPUT (Universal Forwarder or Heavy Forwarder)                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Read data from source (file, syslog, API)                                 â”‚
â”‚ â€¢ Assign metadata: source, sourcetype, host, index                          â”‚
â”‚ â€¢ NO parsing here on Universal Forwarder                                    â”‚
â”‚ â€¢ Heavy Forwarder CAN parse here (reduces indexer load)                     â”‚
â”‚ Config: inputs.conf                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: PARSING (Heavy Forwarder OR Indexer)                               â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ LINE BREAKING: Split raw stream into individual events                    â”‚
â”‚ â€¢ TIMESTAMP EXTRACTION: Find and parse _time                                â”‚
â”‚ â€¢ TRANSFORMS: Route, filter, mask, modify                                   â”‚
â”‚ Config: props.conf + transforms.conf                                        â”‚
â”‚                                                                             â”‚
â”‚ âš ï¸ CRITICAL DECISION: Parse at Heavy Forwarder or Indexer?                  â”‚
â”‚    â€¢ Heavy Forwarder: Reduces network traffic, distributes load             â”‚
â”‚    â€¢ Indexer: Simpler architecture, centralized config                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 3: INDEXING (Indexer only)                                            â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ SEGMENTATION: Break text into searchable tokens                           â”‚
â”‚ â€¢ INDEX-TIME FIELD EXTRACTION: Extract fields, store with event             â”‚
â”‚ â€¢ COMPRESSION: Compress and write to buckets                                â”‚
â”‚ Config: props.conf (INDEXED_EXTRACTIONS), indexes.conf                      â”‚
â”‚                                                                             â”‚
â”‚ âš ï¸ Index-time extraction: Faster search, but increases storage              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 4: SEARCH-TIME (Search Head)                                          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ SEARCH-TIME FIELD EXTRACTION: Extract fields when queried                 â”‚
â”‚ â€¢ LOOKUPS: Enrich with external data                                        â”‚
â”‚ â€¢ CALCULATED FIELDS: Derive new fields via eval                             â”‚
â”‚ Config: props.conf (EXTRACT-, REPORT-), transforms.conf                     â”‚
â”‚                                                                             â”‚
â”‚ âœ… Default approach: Flexible, no storage overhead                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>
        </div>

        <h3>3.2 Heavy Forwarder vs Indexer Parsing â€” Decision Guide</h3>

        <div class="table-wrapper">
          <table>
            <thead>
              <tr><th>Factor</th><th>Parse at Heavy Forwarder</th><th>Parse at Indexer</th></tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>Network traffic</strong></td>
                <td>âœ… Reduced â€” can filter/drop before sending</td>
                <td>âŒ Full raw data sent over network</td>
              </tr>
              <tr>
                <td><strong>Indexer load</strong></td>
                <td>âœ… Reduced â€” parsing already done</td>
                <td>âŒ Indexer does all parsing work</td>
              </tr>
              <tr>
                <td><strong>Configuration management</strong></td>
                <td>âŒ Must maintain configs on multiple HFs</td>
                <td>âœ… Centralized config on indexers</td>
              </tr>
              <tr>
                <td><strong>Troubleshooting</strong></td>
                <td>âŒ Harder â€” must check HF logs</td>
                <td>âœ… Easier â€” all parsing in one place</td>
              </tr>
              <tr>
                <td><strong>Use case</strong></td>
                <td>Syslog aggregation, high-volume sources, data masking</td>
                <td>Standard deployments, simpler architecture</td>
              </tr>
            </tbody>
          </table>
        </div>

        <h3>3.3 Step-by-Step: Parsing Palo Alto Logs at Heavy Forwarder</h3>

        <div class="callout critical">
          <div class="callout-title"><svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm1 15h-2v-6h2v6zm0-8h-2V7h2v2z"/></svg>Scenario</div>
          <div class="callout-content">
            <p>Palo Alto firewalls send syslog to Heavy Forwarder. We want to:</p>
            <ol style="margin: 0.5rem 0; padding-left: 1.5rem;">
              <li>Receive syslog on UDP 514</li>
              <li>Identify log type (TRAFFIC, THREAT, SYSTEM)</li>
              <li>Route to different indexes based on type</li>
              <li>Extract key fields at Heavy Forwarder (reduce indexer load)</li>
              <li>Drop health check logs (save license)</li>
            </ol>
          </div>
        </div>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Step 1: inputs.conf (Heavy Forwarder) â€” Receive Syslog</span><button class="code-copy-btn">Copy</button></div>
          <pre><code># /opt/splunk/etc/apps/TA-paloalto/local/inputs.conf

[udp://514]
connection_host = ip
sourcetype = pan:log
index = network_raw
disabled = false

# What this does:
# â€¢ Listen on UDP 514
# â€¢ Set sourcetype to pan:log (triggers props.conf stanzas)
# â€¢ connection_host = ip means use sender IP as "host" field
# â€¢ Temporarily store in network_raw (will be rerouted)</code></pre>
        </div>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Step 2: props.conf (Heavy Forwarder) â€” Parse and Route</span><button class="code-copy-btn">Copy</button></div>
          <pre><code># /opt/splunk/etc/apps/TA-paloalto/local/props.conf

[pan:log]
# === LINE BREAKING ===
# Each syslog line is one event
SHOULD_LINEMERGE = false
LINE_BREAKER = ([\r\n]+)
TRUNCATE = 65535

# === TIMESTAMP ===
TIME_PREFIX = ^[A-Z][a-z]{2}\s+\d+\s+\d+:\d+:\d+\s+\S+\s+
TIME_FORMAT = %Y/%m/%d %H:%M:%S
MAX_TIMESTAMP_LOOKAHEAD = 32

# === TRANSFORMS (applied in order) ===
# Route to different indexes based on log type
TRANSFORMS-route_traffic = pan_route_traffic
TRANSFORMS-route_threat = pan_route_threat
TRANSFORMS-route_system = pan_route_system

# Drop health check logs
TRANSFORMS-drop_health = pan_drop_healthcheck

# Change sourcetype based on log type
TRANSFORMS-sourcetype_traffic = pan_sourcetype_traffic
TRANSFORMS-sourcetype_threat = pan_sourcetype_threat

# === FIELD EXTRACTION (at Heavy Forwarder = index-time) ===
# Extract fields NOW to reduce indexer load
TRANSFORMS-extract_fields = pan_extract_csv

# What this does:
# 1. Each line becomes one event (no merging)
# 2. Finds timestamp in Palo Alto format
# 3. Applies transforms to route, filter, and extract</code></pre>
        </div>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Step 3: transforms.conf (Heavy Forwarder) â€” Route and Filter</span><button class="code-copy-btn">Copy</button></div>
          <pre><code># /opt/splunk/etc/apps/TA-paloalto/local/transforms.conf

# === ROUTING TRANSFORMS ===
# Route TRAFFIC logs to network_traffic index
[pan_route_traffic]
REGEX = ,TRAFFIC,
DEST_KEY = _MetaData:Index
FORMAT = network_traffic

# Route THREAT logs to network_threat index
[pan_route_threat]
REGEX = ,THREAT,
DEST_KEY = _MetaData:Index
FORMAT = network_threat

# Route SYSTEM logs to network_system index
[pan_route_system]
REGEX = ,SYSTEM,
DEST_KEY = _MetaData:Index
FORMAT = network_system

# === FILTERING TRANSFORM ===
# Drop health check/heartbeat logs (save license!)
[pan_drop_healthcheck]
REGEX = heartbeat|health[-_]?check|keepalive
DEST_KEY = queue
FORMAT = nullQueue

# === SOURCETYPE TRANSFORMS ===
# More specific sourcetype for downstream parsing
[pan_sourcetype_traffic]
REGEX = ,TRAFFIC,
DEST_KEY = MetaData:Sourcetype
FORMAT = pan:traffic

[pan_sourcetype_threat]
REGEX = ,THREAT,
DEST_KEY = MetaData:Sourcetype
FORMAT = pan:threat

# === FIELD EXTRACTION (CSV) ===
# Palo Alto logs are CSV - extract key fields at index time
[pan_extract_csv]
DELIMS = ","
FIELDS = "future_use1","receive_time","serial_number","type","threat_content_type",\
         "future_use2","generated_time","src_ip","dst_ip","nat_src_ip","nat_dst_ip",\
         "rule_name","src_user","dst_user","app","vsys","src_zone","dst_zone",\
         "ingress_if","egress_if","log_profile","future_use3","session_id","repeat_count",\
         "src_port","dst_port","nat_src_port","nat_dst_port","flags","protocol","action"

# What this does:
# â€¢ TRAFFIC logs â†’ network_traffic index with sourcetype pan:traffic
# â€¢ THREAT logs â†’ network_threat index with sourcetype pan:threat
# â€¢ Health checks â†’ nullQueue (dropped, never indexed)
# â€¢ CSV fields extracted at Heavy Forwarder (index-time)</code></pre>
        </div>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Step 4: outputs.conf (Heavy Forwarder) â€” Forward to Indexers</span><button class="code-copy-btn">Copy</button></div>
          <pre><code># /opt/splunk/etc/apps/TA-paloalto/local/outputs.conf

[tcpout]
defaultGroup = indexer_cluster

[tcpout:indexer_cluster]
server = indexer1.revflow.com:9997, indexer2.revflow.com:9997, indexer3.revflow.com:9997
useACK = true
compressed = true

# What happens now:
# 1. Parsed events (with extracted fields) are forwarded
# 2. Health checks are already dropped (never sent)
# 3. Compressed transmission reduces bandwidth
# 4. useACK ensures delivery confirmation</code></pre>
        </div>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Step 5: Verify Parsing â€” SPL Query</span><button class="code-copy-btn">Copy</button></div>
          <pre><code># Test the parsing worked
index=network_traffic sourcetype=pan:traffic earliest=-15m
| head 10
| table _time, src_ip, dst_ip, src_port, dst_port, action, app, rule_name

# Check field extraction
index=network_traffic sourcetype=pan:traffic earliest=-1h
| fieldsummary
| where count > 0
| table field, count, distinct_count

# Verify health checks were dropped
index=* sourcetype=pan:* "healthcheck" earliest=-1h
| stats count
# Should return 0 if filtering is working</code></pre>
        </div>

        <h3>3.4 Search-Time Parsing (Alternative Approach)</h3>

        <p>Instead of extracting fields at Heavy Forwarder, you can extract at search time:</p>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">props.conf â€” Search-Time Extraction</span><button class="code-copy-btn">Copy</button></div>
          <pre><code># Search-time extraction (default, flexible approach)
[pan:traffic]
# No TRANSFORMS for field extraction
# Instead, use EXTRACT- or REPORT-

# Regex-based extraction (search-time)
EXTRACT-src_ip = src=(?&lt;src_ip&gt;\d+\.\d+\.\d+\.\d+)
EXTRACT-dst_ip = dst=(?&lt;dst_ip&gt;\d+\.\d+\.\d+\.\d+)
EXTRACT-action = action=(?&lt;action&gt;\w+)

# Or reference transforms.conf for complex patterns
REPORT-pan_fields = pan_field_extraction

# Field aliases for CIM compliance
FIELDALIAS-dest = dst_ip AS dest
FIELDALIAS-src = src_ip AS src

# Calculated field
EVAL-bytes_total = bytes_in + bytes_out

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# SEARCH-TIME PROS:
# â€¢ Flexible â€” can modify extraction without re-indexing
# â€¢ No storage overhead for extracted fields
# â€¢ Easier to fix mistakes
#
# SEARCH-TIME CONS:
# â€¢ Slower searches (extraction happens every query)
# â€¢ CPU cost at search time
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•</code></pre>
        </div>

        <!-- ==================== SECTION 4: SENTINEL PARSING DEEP DIVE ==================== -->
        <h2>4. Sentinel Parsing â€” Step by Step</h2>

        <h3>4.1 The Three Parsing Approaches</h3>

        <div class="flow-diagram">
          <pre><code>RAW LOG DATA
     â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚                                                                         â”‚
     â–¼                                                                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ APPROACH 1: NATIVE CONNECTOR (Automatic Parsing)                            â”‚â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚â”‚
â”‚ â€¢ Microsoft-provided connectors for common sources                          â”‚â”‚
â”‚ â€¢ Parsing is pre-built â€” you configure, Microsoft parses                    â”‚â”‚
â”‚ â€¢ Data lands in standard tables with defined schemas                        â”‚â”‚
â”‚                                                                             â”‚â”‚
â”‚ Examples: Microsoft 365, Azure AD, AWS CloudTrail, Palo Alto (via CEF)      â”‚â”‚
â”‚ Config: Enable connector in Sentinel UI, provide credentials                â”‚â”‚
â”‚                                                                             â”‚â”‚
â”‚ âœ… Easiest approach â€” zero parsing code                                     â”‚â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                                                                               â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APPROACH 2: DCR INGESTION-TIME TRANSFORMATION                               â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Parse during ingestion using KQL in DCR                                   â”‚
â”‚ â€¢ Transform before data is written to table                                 â”‚
â”‚ â€¢ Filter, enrich, route at ingestion                                        â”‚
â”‚                                                                             â”‚
â”‚ Use when: Custom syslog sources, need pre-ingestion filtering               â”‚
â”‚ Config: Data Collection Rule with transformKql                              â”‚
â”‚                                                                             â”‚
â”‚ âš¡ Near real-time, reduces query-time processing                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APPROACH 3: AZURE FUNCTION (Pre-Ingestion Processing)                       â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Custom code (Python/C#/PowerShell) processes logs                         â”‚
â”‚ â€¢ Full programming flexibility â€” any transformation                         â”‚
â”‚ â€¢ Receives logs via HTTP, sends to Log Analytics API                        â”‚
â”‚                                                                             â”‚
â”‚ Use when: Complex parsing, API-based sources, multi-step transforms         â”‚
â”‚ Config: Azure Function App + Log Analytics Data Collector API               â”‚
â”‚                                                                             â”‚
â”‚ ğŸ”§ Most flexible, most complex                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚
     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ APPROACH 4: KQL QUERY-TIME PARSING                                          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â€¢ Parse when querying using KQL functions                                   â”‚
â”‚ â€¢ Data stored raw, parsed on demand                                         â”‚
â”‚ â€¢ Use parse, extract, split operators                                       â”‚
â”‚                                                                             â”‚
â”‚ Use when: Ad-hoc analysis, prototyping, data already ingested               â”‚
â”‚ Config: KQL in queries, workbooks, analytics rules                          â”‚
â”‚                                                                             â”‚
â”‚ ğŸ“Š Flexible but slower for large datasets                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</code></pre>
        </div>

        <h3>4.2 Step-by-Step: DCR Ingestion-Time Parsing</h3>

        <div class="callout critical">
          <div class="callout-title"><svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M12 2C6.48 2 2 6.48 2 12s4.48 10 10 10 10-4.48 10-10S17.52 2 12 2zm1 15h-2v-6h2v6zm0-8h-2V7h2v2z"/></svg>Scenario</div>
          <div class="callout-content">
            <p>Custom application sends JSON logs. We want to:</p>
            <ol style="margin: 0.5rem 0; padding-left: 1.5rem;">
              <li>Receive logs via AMA</li>
              <li>Parse JSON fields</li>
              <li>Filter out DEBUG logs (save costs)</li>
              <li>Route to custom table MyApp_CL</li>
            </ol>
          </div>
        </div>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Step 1: Create Custom Table (Log Analytics)</span><button class="code-copy-btn">Copy</button></div>
          <pre><code># Create custom table via Azure CLI or REST API

# Define table schema
az monitor log-analytics workspace table create \
  --resource-group "rg-sentinel-prod" \
  --workspace-name "law-sentinel-prod" \
  --name "MyApp_CL" \
  --columns \
    TimeGenerated=datetime \
    Level=string \
    Message=string \
    UserId=string \
    Action=string \
    SourceIP=string \
    Duration=real \
    Success=boolean

# This creates:
# â€¢ Custom table MyApp_CL in Log Analytics
# â€¢ Schema defines field names and types
# â€¢ Table is ready to receive data</code></pre>
        </div>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Step 2: Create Data Collection Endpoint (DCE)</span><button class="code-copy-btn">Copy</button></div>
          <pre><code># Create DCE for custom log ingestion
az monitor data-collection endpoint create \
  --resource-group "rg-sentinel-prod" \
  --name "dce-myapp-logs" \
  --location "eastus" \
  --public-network-access "Enabled"

# Output includes:
# â€¢ Logs ingestion endpoint URL
# â€¢ Resource ID for DCR reference</code></pre>
        </div>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Step 3: Create DCR with Transformation â€” Full JSON</span><button class="code-copy-btn">Copy</button></div>
          <pre><code>{
  "location": "eastus",
  "properties": {
    "dataCollectionEndpointId": "/subscriptions/.../dataCollectionEndpoints/dce-myapp-logs",
    
    "streamDeclarations": {
      "Custom-MyAppRaw": {
        "columns": [
          { "name": "TimeGenerated", "type": "datetime" },
          { "name": "RawData", "type": "string" }
        ]
      }
    },
    
    "dataSources": {
      "logFiles": [
        {
          "name": "myapp-json-logs",
          "streams": ["Custom-MyAppRaw"],
          "filePatterns": ["/var/log/myapp/*.json"],
          "format": "text"
        }
      ]
    },
    
    "destinations": {
      "logAnalytics": [
        {
          "workspaceResourceId": "/subscriptions/.../workspaces/law-sentinel-prod",
          "name": "law-destination"
        }
      ]
    },
    
    "dataFlows": [
      {
        "streams": ["Custom-MyAppRaw"],
        "destinations": ["law-destination"],
        "transformKql": "source 
          | extend parsed = parse_json(RawData)
          | extend 
              Level = tostring(parsed.level),
              Message = tostring(parsed.message),
              UserId = tostring(parsed.user_id),
              Action = tostring(parsed.action),
              SourceIP = tostring(parsed.source_ip),
              Duration = todouble(parsed.duration_ms),
              Success = tobool(parsed.success)
          | where Level != 'DEBUG'
          | project 
              TimeGenerated,
              Level,
              Message,
              UserId,
              Action,
              SourceIP,
              Duration,
              Success",
        "outputStream": "Custom-MyApp_CL"
      }
    ]
  }
}

// What transformKql does:
// 1. parse_json(RawData) â€” Parse JSON string into object
// 2. extend â€” Extract fields from JSON
// 3. where Level != 'DEBUG' â€” Filter out DEBUG logs (saves cost!)
// 4. project â€” Select only fields we want in final table</code></pre>
        </div>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Step 4: Deploy DCR and Associate with VMs</span><button class="code-copy-btn">Copy</button></div>
          <pre><code># Create DCR from JSON file
az monitor data-collection rule create \
  --resource-group "rg-sentinel-prod" \
  --name "dcr-myapp-parser" \
  --location "eastus" \
  --rule-file "./dcr-myapp.json"

# Associate DCR with VM (AMA must be installed)
az monitor data-collection rule association create \
  --name "myapp-assoc" \
  --resource "/subscriptions/.../virtualMachines/vm-app01" \
  --rule-id "/subscriptions/.../dataCollectionRules/dcr-myapp-parser"

# Verify data is flowing
az monitor log-analytics query \
  --workspace "law-sentinel-prod" \
  --analytics-query "MyApp_CL | take 10"</code></pre>
        </div>

        <h3>4.3 Step-by-Step: Azure Function Parsing</h3>

        <div class="callout warning">
          <div class="callout-title"><svg width="16" height="16" viewBox="0 0 24 24" fill="currentColor"><path d="M1 21h22L12 2 1 21zm12-3h-2v-2h2v2zm0-4h-2v-4h2v4z"/></svg>When to Use Azure Function</div>
          <div class="callout-content">
            <p>Use Azure Functions when:</p>
            <ul style="margin: 0.5rem 0; padding-left: 1.5rem;">
              <li>Source sends data via API (webhook, REST)</li>
              <li>Complex multi-step transformations</li>
              <li>Need to call external APIs for enrichment</li>
              <li>KQL transformation is insufficient</li>
            </ul>
          </div>
        </div>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Azure Function â€” Python Log Parser</span><button class="code-copy-btn">Copy</button></div>
          <pre><code># function_app.py
import azure.functions as func
import logging
import json
import hashlib
import hmac
import base64
import datetime
import requests

app = func.FunctionApp()

# Log Analytics workspace details
WORKSPACE_ID = "your-workspace-id"
SHARED_KEY = "your-shared-key"
LOG_TYPE = "CustomAppLogs"  # Will create CustomAppLogs_CL table

def build_signature(workspace_id, shared_key, date, content_length, method, content_type, resource):
    """Build authorization signature for Log Analytics API"""
    x_headers = f'x-ms-date:{date}'
    string_to_hash = f'{method}\n{str(content_length)}\n{content_type}\n{x_headers}\n{resource}'
    bytes_to_hash = bytes(string_to_hash, encoding="utf-8")
    decoded_key = base64.b64decode(shared_key)
    encoded_hash = base64.b64encode(
        hmac.new(decoded_key, bytes_to_hash, digestmod=hashlib.sha256).digest()
    ).decode()
    return f'SharedKey {workspace_id}:{encoded_hash}'

def post_to_log_analytics(body):
    """Send parsed logs to Log Analytics"""
    method = 'POST'
    content_type = 'application/json'
    resource = '/api/logs'
    rfc1123date = datetime.datetime.utcnow().strftime('%a, %d %b %Y %H:%M:%S GMT')
    content_length = len(body)
    
    signature = build_signature(
        WORKSPACE_ID, SHARED_KEY, rfc1123date, 
        content_length, method, content_type, resource
    )
    
    uri = f'https://{WORKSPACE_ID}.ods.opinsights.azure.com{resource}?api-version=2016-04-01'
    
    headers = {
        'content-type': content_type,
        'Authorization': signature,
        'Log-Type': LOG_TYPE,
        'x-ms-date': rfc1123date
    }
    
    response = requests.post(uri, data=body, headers=headers)
    return response.status_code

@app.function_name(name="LogParser")
@app.route(route="logs", methods=["POST"])
def log_parser(req: func.HttpRequest) -> func.HttpResponse:
    """
    Receive logs via HTTP POST, parse, and send to Log Analytics
    """
    logging.info('Log parser function triggered')
    
    try:
        # Get raw log data
        raw_logs = req.get_json()
        
        parsed_logs = []
        
        for log in raw_logs:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # PARSING LOGIC â€” Customize this section
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            
            # Skip DEBUG logs
            if log.get('level') == 'DEBUG':
                continue
            
            # Parse and transform
            parsed_log = {
                'TimeGenerated': log.get('timestamp', datetime.datetime.utcnow().isoformat()),
                'Level': log.get('level', 'INFO'),
                'Message': log.get('message', ''),
                'UserId': log.get('user_id', ''),
                'Action': log.get('action', ''),
                'SourceIP': log.get('source_ip', ''),
                'Duration': float(log.get('duration_ms', 0)),
                'Success': bool(log.get('success', True)),
                
                # Enrichment â€” add derived fields
                'Hour': datetime.datetime.fromisoformat(
                    log.get('timestamp', datetime.datetime.utcnow().isoformat())
                ).hour,
                'IsBusinessHours': 9 <= datetime.datetime.fromisoformat(
                    log.get('timestamp', datetime.datetime.utcnow().isoformat())
                ).hour < 17
            }
            
            # GeoIP enrichment example (call external API)
            if parsed_log['SourceIP']:
                # geo = get_geoip(parsed_log['SourceIP'])
                # parsed_log['Country'] = geo.get('country', 'Unknown')
                pass
            
            parsed_logs.append(parsed_log)
        
        # Send to Log Analytics
        if parsed_logs:
            body = json.dumps(parsed_logs)
            status_code = post_to_log_analytics(body)
            
            if status_code == 200:
                return func.HttpResponse(
                    f"Successfully processed {len(parsed_logs)} logs",
                    status_code=200
                )
            else:
                return func.HttpResponse(
                    f"Error sending to Log Analytics: {status_code}",
                    status_code=500
                )
        
        return func.HttpResponse("No logs to process", status_code=200)
        
    except Exception as e:
        logging.error(f"Error processing logs: {str(e)}")
        return func.HttpResponse(f"Error: {str(e)}", status_code=500)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEPLOYMENT:
# 1. Create Function App in Azure
# 2. Deploy this code
# 3. Configure WORKSPACE_ID and SHARED_KEY in App Settings
# 4. Point log source to Function URL: https://yourfunc.azurewebsites.net/api/logs
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•</code></pre>
        </div>

        <h3>4.4 Step-by-Step: KQL Query-Time Parsing</h3>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">KQL â€” Parsing Raw Syslog Data</span><button class="code-copy-btn">Copy</button></div>
          <pre><code>// Scenario: Raw syslog in Syslog table, need to parse SSH auth logs
Syslog
| where Facility == "authpriv" and ProcessName == "sshd"
| where TimeGenerated > ago(1h)

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// METHOD 1: parse operator â€” Pattern matching
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
| parse SyslogMessage with * "Failed password for " UserType " user " Username 
                            " from " SourceIP " port " SourcePort:int *
| where isnotempty(Username)

// Result: UserType, Username, SourceIP, SourcePort extracted as new columns

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// METHOD 2: parse-where â€” Parse and filter in one step
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
| parse-where SyslogMessage with * "Accepted " AuthMethod " for " Username 
                                  " from " SourceIP " port " SourcePort:int *
// Only keeps rows that match the pattern

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// METHOD 3: extract â€” Regex extraction
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
| extend SourceIP = extract(@"from\s+(\d+\.\d+\.\d+\.\d+)", 1, SyslogMessage)
| extend Username = extract(@"for\s+(?:invalid\s+user\s+)?(\S+)", 1, SyslogMessage)
| extend SourcePort = toint(extract(@"port\s+(\d+)", 1, SyslogMessage))

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// METHOD 4: split â€” Delimiter-based parsing
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// For CSV-like data
| extend Fields = split(SyslogMessage, ",")
| extend 
    Field1 = tostring(Fields[0]),
    Field2 = tostring(Fields[1]),
    Field3 = toint(Fields[2])

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// METHOD 5: parse_json â€” JSON parsing
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
| extend ParsedJson = parse_json(SyslogMessage)
| extend 
    Level = tostring(ParsedJson.level),
    Message = tostring(ParsedJson.message),
    UserId = tostring(ParsedJson.user_id)

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// COMPLETE EXAMPLE: SSH Failed Login Parser
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
Syslog
| where Facility == "authpriv" and ProcessName == "sshd"
| where SyslogMessage contains "Failed password"
| parse SyslogMessage with * "Failed password for " * "user " Username 
                            " from " SourceIP " port " SourcePort:int *
| extend 
    IsInvalidUser = SyslogMessage contains "invalid user",
    AttemptType = iff(SyslogMessage contains "invalid user", "Invalid User", "Valid User")
| summarize 
    FailedAttempts = count(),
    UniqueUsers = dcount(Username),
    UserList = make_set(Username, 10)
    by SourceIP, bin(TimeGenerated, 1h)
| where FailedAttempts > 5
| order by FailedAttempts desc</code></pre>
        </div>

        <!-- ==================== SECTION 5: COMPARISON SUMMARY ==================== -->
        <h2>5. Parsing Approach Comparison</h2>

        <div class="table-wrapper">
          <table>
            <thead>
              <tr><th>Aspect</th><th>Splunk (HF/Indexer)</th><th>Sentinel (DCR)</th><th>Sentinel (Function)</th><th>Sentinel (KQL)</th></tr>
            </thead>
            <tbody>
              <tr>
                <td><strong>When parsing happens</strong></td>
                <td>Index-time or search-time</td>
                <td>Ingestion-time</td>
                <td>Pre-ingestion</td>
                <td>Query-time</td>
              </tr>
              <tr>
                <td><strong>Language</strong></td>
                <td>Regex in props.conf</td>
                <td>KQL in DCR</td>
                <td>Python/C#/PowerShell</td>
                <td>KQL</td>
              </tr>
              <tr>
                <td><strong>Flexibility</strong></td>
                <td>High</td>
                <td>Medium</td>
                <td>Highest</td>
                <td>High</td>
              </tr>
              <tr>
                <td><strong>Performance</strong></td>
                <td>Fast (index-time) / Slower (search-time)</td>
                <td>Fast</td>
                <td>Depends on code</td>
                <td>Slower for large data</td>
              </tr>
              <tr>
                <td><strong>Storage impact</strong></td>
                <td>Index-time increases storage</td>
                <td>None (parsed before storage)</td>
                <td>None</td>
                <td>None</td>
              </tr>
              <tr>
                <td><strong>Modify after deployment</strong></td>
                <td>Index-time: No / Search-time: Yes</td>
                <td>Yes (new data only)</td>
                <td>Yes (redeploy function)</td>
                <td>Yes (change query)</td>
              </tr>
              <tr>
                <td><strong>Best for</strong></td>
                <td>Enterprise control</td>
                <td>Standard custom logs</td>
                <td>Complex/API sources</td>
                <td>Ad-hoc, prototyping</td>
              </tr>
            </tbody>
          </table>
        </div>

        <!-- ==================== SECTION 6: INTERVIEW ANSWERS ==================== -->
        <h2>6. Interview-Ready Answers</h2>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Q: "Explain when you'd parse at Heavy Forwarder vs Indexer in Splunk"</span></div>
          <pre><code>"I parse at the Heavy Forwarder when I need to:
1. Reduce network traffic â€” filter or drop events before they're sent
2. Mask sensitive data â€” PII masking must happen before indexing
3. Route events â€” send different log types to different indexes
4. Distribute parsing load â€” reduce indexer CPU for high-volume sources

I parse at the Indexer when:
1. Simpler architecture is preferred â€” centralized configuration
2. Low-volume sources where HF overhead isn't justified
3. Easier troubleshooting â€” all parsing logic in one place

For RevFlow, we used Heavy Forwarders for network devices (Palo Alto, Cisco)
because they generate 150+ GB/day. HF filtered health checks and routed 
TRAFFIC vs THREAT logs to separate indexes. Saved significant license cost."</code></pre>
        </div>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Q: "When would you use Azure Function vs DCR transformation in Sentinel?"</span></div>
          <pre><code>"DCR transformation is my default choice because:
1. Native to Azure â€” no separate infrastructure to manage
2. KQL is powerful enough for most parsing needs
3. Lower cost â€” no Function App consumption charges
4. Simpler â€” transformation defined in the same DCR as collection

I use Azure Functions when:
1. Source sends data via API/webhook â€” DCR can't receive HTTP
2. Need external enrichment â€” call GeoIP or threat intel APIs during parsing
3. Complex multi-step transformations â€” KQL has limits on complexity
4. Need to aggregate before ingestion â€” batch processing logic

For RevFlow, we used DCR transformations for custom Linux apps (JSON logs).
We used Azure Function for a third-party SaaS that sent webhooks â€”
the Function received HTTP, parsed, enriched with user info from 
Graph API, then sent to Log Analytics."</code></pre>
        </div>

        <div class="code-block">
          <div class="code-header"><span class="code-lang">Q: "What's the difference between index-time and search-time extraction?"</span></div>
          <pre><code>"Index-time extraction happens when data is written to disk. Fields are 
stored with the event. Search-time extraction happens when you query.

Index-time pros:
â€¢ Faster searches â€” fields already extracted
â€¢ Can route/filter based on extracted fields

Index-time cons:
â€¢ Increases storage â€” extracted fields take space
â€¢ Inflexible â€” can't change after indexing
â€¢ Must re-ingest to fix mistakes

Search-time pros:
â€¢ Flexible â€” modify extractions anytime
â€¢ No storage overhead
â€¢ Can iterate on extraction logic

Search-time cons:
â€¢ Slower â€” extraction happens every query
â€¢ CPU cost at search time

My default is search-time for most fields. I only use index-time for:
1. High-volume fields I search constantly (src_ip, action)
2. Fields needed for routing (to decide which index)
3. Fields needed for filtering before storage

In Sentinel, DCR transformation is similar to index-time â€” it happens 
at ingestion. KQL parsing is search-time. There's no equivalent to 
Splunk's stored index-time fields; Sentinel doesn't store extracted 
fields separately."</code></pre>
        </div>

        <nav class="page-nav">
          <a href="log-engineering.html" class="page-nav-link prev"><span class="page-nav-label">Previous</span><span class="page-nav-title">â† Log Engineering</span></a>
          <a href="threat-intelligence.html" class="page-nav-link next"><span class="page-nav-label">Next</span><span class="page-nav-title">Threat Intelligence â†’</span></a>
        </nav>
      </div>
    </main>
  </div>
  <script src="../js/main.js"></script>
</body>
</html>
